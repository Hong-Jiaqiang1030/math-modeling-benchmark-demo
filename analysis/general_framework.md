### **大模型数学建模能力三层评估基准（Three-Tier Benchmark for LLMs' Mathematical Modeling Capability）**

#### **顶层架构：遵循认知路径的三层递进式评估**

本基准严格遵循数学建模的认知路径——**情景模型 → 真实模型 → 数学模型**，将其转化为三个逻辑递进、各有侧重的评估模块。此设计系统性地考察大模型从理解初始问题到构建并验证最终数学解决方案的全过程能力。

| 评估层级 | 对应认知阶段 | 核心评估目标 | 任务性质 | 验证方式 |
| :--- | :--- | :--- | :--- | :--- |
| **第一层** | 情景模型 (Situation Model) | **问题理解能力**：检验模型对非结构化现实问题文本的深度理解和细节捕捉能力。 | 封闭式问答 | 自动化评估 + 专家LLM评估 |
| **第二层** | 真实模型 (Real Model) | **抽象与结构化能力**：检验模型在更高抽象层面把握问题核心、识别关键要素与约束的能力。 | 半开放式结构化输出 | 专家LLM评估（多维度打分） |
| **第三层** | 数学模型 (Mathematical Model) | **形式化与求解能力**：检验模型将抽象的真实模型转化为精确、可执行的数学表达式并求解的能力。 | 开放式 + 封闭式 | 自动化求解器验证 + 专家LLM评审 |

---

#### **模块一：情景模型构建能力评估（问题理解层）**

**1. 任务设计**
- **输入**：一段来自真实世界场景的非结构化文本（如新闻报道、科研论文摘要、商业报告节选）。
- **输出**：针对该文本的一系列精心构造的追问的回答。
- **问题类型**：
  - **事实性问题**：检验基本事实的捕捉能力。（例：“报告中提到的A区人口密度是多少？”）
  - **关系性问题**：检验对事物间关联的理解。（例：“天气状况与共享单车日均使用量之间存在何种关系？”）
  - **因果性问题**：检验对原因和结果的推断能力。（例：“为什么政府出台了限制B区单车投放的新规？”）

**2. 评估方法**
采用分层评估策略，兼顾效率与深度：

| 评估维度 | 评估方法 | 实现细节与指标 |
| :--- | :--- | :--- |
| **事实一致性** | 自动化问答评估 | **精确匹配(EM)**、**F1分数**。 |
| **细节捕捉度** | 关键信息抽取与匹配 | 计算回答与“黄金标准”答案的**交并比(Jaccard Similarity)**。 |
| **因果关系理解** | 专家LLM评估 | 由高质量LLM（如o1-preview）作为“评委”，从**准确性、完整性、逻辑性**三个维度打分（1-5分）。 |
| **整体情境连贯性** | 自然语言推理(NLI) | 将模型回答重组为叙述，用NLI模型判断其与原文的逻辑关系（蕴含/矛盾/无关）。 |

**3. 执行流程**
自动化流水线：展示文本 → 提出问题 → 模型生成回答 → 自动化模块评估事实性/细节性问题 → 专家LLM模块评估因果/连贯性问题 → 生成综合报告。

---

#### **模块二：真实模型构建能力评估（抽象与结构化层）**

**1. 任务设计**
- **输入**：与模块一相同的复杂现实场景描述。
- **输出**：更高阶的思维产物，形式包括：
  - **要素清单**：列出解决问题必需的**关键变量、核心假设、主要约束**。
  - **概念图**：以文本形式（如节点-关系三元组）呈现问题的**结构化框架**。

**2. 评估方法**
鉴于真实模型构建的开放性，评估以**专家LLM评估为核心**，辅以自动化辅助。

- **自动化辅助评估**：
  - **要素清单**：与预设的“黄金标准”清单计算**交并比(Jaccard Similarity)**，评估完备性。
  - **概念图**：通过模板匹配或检查节点/边的关系进行初步评估。

- **专家LLM评估（核心）**：
  组织一个或多个高质量LLM扮演领域专家，依据以下维度对输出进行打分（1-5分）：
  - **合理性 (Reasonableness)**：所提假设和变量是否符合常理和领域知识？
  - **完备性 (Completeness)**：是否遗漏了影响问题解决的关键因素？
  - **简洁性与抽象性 (Simplicity & Abstraction)**：是否抓住了问题本质，避免了不必要的细节纠缠？
  - **创新性 (Innovativeness)**：是否提出了独特的视角或关键变量？

**3. 执行流程**
模型接收任务 → 生成要素清单或概念图 → 系统调用专家LLM评估模块 → 专家LLM依据评分量表给出各维度分数 → 汇总生成该模块的综合得分。

---

#### **模块三：数学模型构建与求解能力评估（形式化与求解层）**

本模块融合Mamo的严谨验证与ModelingBench的开放性，设置两类任务以全面评估。

**1. 任务设计**
- **封闭式任务 (Closed-ended Tasks)**：
  - **来源**：具有明确数学形式和唯一解的问题（如物理运动、经典资源分配）。
  - **输出要求**：强制以**LaTeX公式**或**Python代码**形式输出完整的数学模型（含目标函数、约束、变量定义）。
- **开放式任务 (Open-ended Tasks)**：
  - **来源**：复杂的、无标准答案的现实问题（如“预测某地区未来十年房价走势”）。
  - **输出要求**：提交一份**完整的建模报告**，涵盖问题重述、假设、变量定义、模型构建、求解、分析、结论等。

**2. 评估方法**
针对不同任务类型，采用差异化的评估策略。

- **对于封闭式任务**：
  - **自动化验证**：将模型生成的代码或公式交给专业外部求解器（如Gurobi, SciPy）执行。
  - **评判标准**：采用Mamo的**相对误差阈值法**（如 `abs((output - standard_answer) / standard_answer) < 1e-4`）判断结果正确性。此方法将评估焦点从模型自身的计算能力转移到其**模型构建的正确性**上。

- **对于开放式任务**：
  - **多维度专家评审**：采用类似ModelingBench的**ModelingJudge**框架。
  - **评审团构成**：组织一个多角色LLM评审团，包括**数学建模专家、领域专家、数据科学家、创新思维顾问**。
  - **评估维度**：
    - **模型的根基 (Groundedness of Modeling)**：模型是否紧密建立在模块二构建的合理真实模型之上？
    - **数据的根基 (Groundedness of Data)**：数据来源是否可靠？处理是否恰当？
    - **分析质量 (Analysis Quality)**：结果分析是否深入、有见地？
    - **创新性 (Innovativeness)**：模型是否有新颖之处？
    - **实用性 (Practicality)**：模型是否清晰、易于复现和应用？

**3. 执行流程**
模型根据任务类型生成相应输出 → 封闭式任务进入自动化求解验证流水线 → 开放式任务提交至多角色LLM评审团 → 评审团依据详细评分标准打分 → 汇总生成该模块的综合得分。
